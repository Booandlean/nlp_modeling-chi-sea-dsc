{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Naive Bayes and NLP Modeling"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Before returning to our Satire/No Satire example, let's consider an example with a smaller but similar scope.\n", "\n", "Suppose we are using an API to gather articles from a news website and grabbing phrases from two different types of articles:  music and politics.\n", "\n", "We have a problem though! Only some of our articles have their category (music or politics). Is there a way we can use Machine Learning to help us label our data quickly?\n", "\n", "-------------------------------\n", "### Here are our articles\n", "#### Music Articles:\n", "\n", "* 'the song was popular'\n", "* 'band leaders disagreed on sound'\n", "* 'played for a sold out arena stadium'\n", "\n", "#### Politics Articles\n", "\n", "* 'world leaders met lask week'\n", "* 'the election was close'\n", "* 'the officials agreed on a compromise'\n", "--------------------------------------------------------\n", "Let's try and predict one example phrase:\n", "\n", "\n", "* \"world leaders agreed to fund the stadium\"\n", "\n", "How can we make a model that labels this for us rather than having to go through by hand?"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from collections import defaultdict\n", "import numpy as np\n", "music = ['the song was popular',\n", "         'band leaders disagreed on sound',\n", "         'played for a sold out arena stadium']\n", "\n", "politics = ['world leaders met lask week',\n", "            'the election was close',\n", "            'the officials agreed on a compromise']\n", "\n", "test_statement = 'world leaders agreed to fund the stadium'"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["#labels : 'music' 'politics'\n", "#features: words\n", "test_statement_2 = 'officials met at the arena'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src =\"./resources/naive_bayes_icon.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Another way of looking at it\n", "<img src = \"./resources/another_one.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## So, in the context of our problem......"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "$\\large P(politics | phrase) = \\frac{P(phrase|politics)P(politics)}{P(phrase)}$\n", "\n", "$\\large P(politics) = \\frac{ \\# politics}{\\# all\\ articles} $\n", "\n", "*where phrase is our test statement*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src = \"./resources/solving_theta.png\" width=\"400\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How should we calculate P(politics)?\n", "\n", "This is essentially the distribution of the probability of either type of article. We have three of each type of article, therefore, we assume that there is an equal probability of either article"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["p_politics = len(politics)/(len(politics) + len(music))\n", "p_music = len(music)/(len(politics) + len(music))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do you think we should calculate: $ P(phrase | politics) $ ?"]}, {"cell_type": "code", "execution_count": 123, "metadata": {}, "outputs": [], "source": ["# we need to break the phrases down into individual words\n"]}, {"cell_type": "markdown", "metadata": {}, "source": [" $\\large P(phrase | politics) = \\prod_{i=1}^{d} P(word_{i} | politics) $"]}, {"cell_type": "code", "execution_count": 127, "metadata": {}, "outputs": [], "source": ["### We need to make a *Naive* assumption."]}, {"cell_type": "markdown", "metadata": {}, "source": [" $\\large P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ art.} {\\#\\ of\\ total\\ words\\ in\\ politics\\ art.} $\n", "\n", "### Can you foresee any issues with this?"]}, {"cell_type": "code", "execution_count": 125, "metadata": {}, "outputs": [], "source": ["# we can't have a probability of 0\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Laplace Smoothing\n", " $\\large P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ art. + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ art. + \\alpha d} $\n", "\n", " $\\large P(word_{i} | music) = \\frac{\\#\\ of\\ word_{i}\\ in\\ music\\ art. + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ music\\ art. + \\alpha d} $"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This correction process is called Laplace smoothing:\n", "* d : number of features (in this instance total number of vocabulary words)\n", "* $\\alpha$ can be any number greater than 0 (it is usually 1)\n", "\n", "\n", "#### Now let's find this calculation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"./resources/IMG_0041.jpg\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["p(phrase|politics)"]}, {"cell_type": "code", "execution_count": 128, "metadata": {}, "outputs": [], "source": ["def vocab_maker(category):\n", "    \"\"\"returns the vocabulary for a given type of article\"\"\"\n", "    vocab_category = set()\n", "    for art in category:\n", "        words = art.split()\n", "        for word in words:\n", "            vocab_category.add(word)\n", "    return vocab_category\n", "        \n", "voc_music = vocab_maker(music)\n", "voc_pol = vocab_maker(politics)\n", "# total_vocabulary = voc_music.union(voc_pol)\n"]}, {"cell_type": "code", "execution_count": 129, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'a',\n", " 'arena',\n", " 'band',\n", " 'disagreed',\n", " 'for',\n", " 'leaders',\n", " 'on',\n", " 'out',\n", " 'played',\n", " 'popular',\n", " 'sold',\n", " 'song',\n", " 'sound',\n", " 'stadium',\n", " 'the',\n", " 'was'}"]}, "execution_count": 129, "metadata": {}, "output_type": "execute_result"}], "source": ["voc_music"]}, {"cell_type": "code", "execution_count": 130, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'a',\n", " 'arena',\n", " 'band',\n", " 'disagreed',\n", " 'for',\n", " 'leaders',\n", " 'on',\n", " 'out',\n", " 'played',\n", " 'popular',\n", " 'sold',\n", " 'song',\n", " 'sound',\n", " 'stadium',\n", " 'the',\n", " 'was'}"]}, "execution_count": 130, "metadata": {}, "output_type": "execute_result"}], "source": ["voc_music"]}, {"cell_type": "code", "execution_count": 131, "metadata": {}, "outputs": [], "source": ["voc_all = voc_music.union(voc_pol)"]}, {"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [], "source": ["total_vocab_count = len(voc_all)\n", "total_music_count = len(voc_music)\n", "total_politics_count = len(voc_pol)"]}, {"cell_type": "code", "execution_count": 133, "metadata": {}, "outputs": [], "source": ["#P(politics | leaders agreed to fund the stadium)"]}, {"cell_type": "code", "execution_count": 134, "metadata": {}, "outputs": [], "source": ["def find_number_words_in_category(phrase,category):\n", "    statement = phrase.split()\n", "    str_category=' '.join(category)\n", "    cat_word_list = str_category.split()\n", "    word_count = defaultdict(int)\n", "    for word in statement:\n", "        for art_word in cat_word_list:\n", "            if word == art_word:\n", "                word_count[word] +=1\n", "            else:\n", "                word_count[word]\n", "    return word_count\n", "                \n", "            "]}, {"cell_type": "code", "execution_count": 135, "metadata": {}, "outputs": [], "source": ["test_music_word_count = find_number_words_in_category(test_statement,music)\n"]}, {"cell_type": "code", "execution_count": 136, "metadata": {}, "outputs": [{"data": {"text/plain": ["defaultdict(int,\n", "            {'world': 0,\n", "             'leaders': 1,\n", "             'agreed': 0,\n", "             'to': 0,\n", "             'fund': 0,\n", "             'the': 1,\n", "             'stadium': 1})"]}, "execution_count": 136, "metadata": {}, "output_type": "execute_result"}], "source": ["test_music_word_count"]}, {"cell_type": "code", "execution_count": 137, "metadata": {}, "outputs": [], "source": ["test_politic_word_count = find_number_words_in_category(test_statement,politics)"]}, {"cell_type": "code", "execution_count": 138, "metadata": {}, "outputs": [{"data": {"text/plain": ["defaultdict(int,\n", "            {'world': 1,\n", "             'leaders': 1,\n", "             'agreed': 1,\n", "             'to': 0,\n", "             'fund': 0,\n", "             'the': 2,\n", "             'stadium': 0})"]}, "execution_count": 138, "metadata": {}, "output_type": "execute_result"}], "source": ["test_politic_word_count"]}, {"cell_type": "code", "execution_count": 139, "metadata": {}, "outputs": [], "source": ["def find_likelihood(category_count,test_category_count,alpha):\n", "    num = np.product(np.array(list(test_category_count.values())) + alpha)\n", "    denom = (category_count + total_vocab_count*alpha)**(len(test_category_count))\n", "    \n", "    return num/denom"]}, {"cell_type": "code", "execution_count": 140, "metadata": {}, "outputs": [], "source": ["likelihood_m = find_likelihood(total_music_count,test_music_word_count,1)"]}, {"cell_type": "code", "execution_count": 141, "metadata": {}, "outputs": [], "source": ["likelihood_p = find_likelihood(total_politics_count,test_politic_word_count,1)"]}, {"cell_type": "code", "execution_count": 142, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["4.107740405680756e-11\n", "1.748875897714495e-10\n"]}], "source": ["print(likelihood_m)\n", "print(likelihood_p)"]}, {"cell_type": "markdown", "metadata": {}, "source": [" $ P(politics | article) = P(politics) x \\prod_{i=1}^{d} P(word_{i} | politics) $"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Deteriming the winner of our model:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src = \"./resources/solvingforyhat.png\" width= \"400\">"]}, {"cell_type": "code", "execution_count": 143, "metadata": {}, "outputs": [], "source": ["p_politics = .5\n", "p_music = .5"]}, {"cell_type": "code", "execution_count": 144, "metadata": {}, "outputs": [{"data": {"text/plain": ["True"]}, "execution_count": 144, "metadata": {}, "output_type": "execute_result"}], "source": ["# p(politics|article)  > p(music|article)\n", "likelihood_p * p_politics  > likelihood_m * p_music"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Many times, the probabilities we end up are exceedingly small, so we can transform them using logs to save on computation speed"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\large log(P(politics | article)) = log(P(politics)) + \\sum_{i=1}^{d}log( P(word_{i} | politics)) $\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Good Resource: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.htmlm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Back to Satire"]}, {"cell_type": "code", "execution_count": 150, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>body</th>\n", "      <th>target</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Noting that the resignation of James Mattis as...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Desperate to unwind after months of nonstop wo...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Nearly halfway through his presidential term, ...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Attempting to make amends for gross abuses of ...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>Decrying the Senate\u2019s resolution blaming the c...</td>\n", "      <td>1</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                                body  target\n", "0  Noting that the resignation of James Mattis as...       1\n", "1  Desperate to unwind after months of nonstop wo...       1\n", "2  Nearly halfway through his presidential term, ...       1\n", "3  Attempting to make amends for gross abuses of ...       1\n", "4  Decrying the Senate\u2019s resolution blaming the c...       1"]}, "execution_count": 150, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "import numpy as np\n", "corpus = pd.read_csv('data/satire_nosatire.csv')\n", "corpus.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Like always, we will perform a train test split..."]}, {"cell_type": "code", "execution_count": 151, "metadata": {}, "outputs": [], "source": ["X=corpus.body\n", "y=corpus.target"]}, {"cell_type": "code", "execution_count": 152, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["...and preprocess the training set like we learned."]}, {"cell_type": "code", "execution_count": 153, "metadata": {}, "outputs": [], "source": ["import nltk\n", "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n", "from nltk.corpus import stopwords"]}, {"cell_type": "code", "execution_count": 154, "metadata": {}, "outputs": [], "source": ["pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n", "token_docs = [regexp_tokenize(doc, pattern) for doc in X_train]\n", "sw = stopwords.words('english')\n", "sw.extend(['would', 'one', 'say'])"]}, {"cell_type": "code", "execution_count": 155, "metadata": {}, "outputs": [], "source": ["from nltk.corpus import wordnet\n", "from nltk import pos_tag\n", "from nltk.stem import WordNetLemmatizer \n", "  \n", "\n", "def get_wordnet_pos(treebank_tag):\n", "    '''\n", "    Translate nltk POS to wordnet tags\n", "    '''\n", "    if treebank_tag.startswith('J'):\n", "        return wordnet.ADJ\n", "    elif treebank_tag.startswith('V'):\n", "        return wordnet.VERB\n", "    elif treebank_tag.startswith('N'):\n", "        return wordnet.NOUN\n", "    elif treebank_tag.startswith('R'):\n", "        return wordnet.ADV\n", "    else:\n", "        return wordnet.NOUN"]}, {"cell_type": "code", "execution_count": 156, "metadata": {}, "outputs": [], "source": ["def doc_preparer(doc, stop_words=sw):\n", "    '''\n", "    \n", "    :param doc: a document from the satire corpus \n", "    :return: a document string with words which have been \n", "            lemmatized, \n", "            parsed for stopwords, \n", "            made lowercase,\n", "            and stripped of punctuation and numbers.\n", "    '''\n", "    \n", "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:\u2019[a-z]+)?)\")\n", "    doc = regex_token.tokenize(doc)\n", "    doc = [word.lower() for word in doc]\n", "    doc = [word for word in doc if word not in stop_words]\n", "    doc = pos_tag(doc)\n", "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n", "    lemmatizer = WordNetLemmatizer() \n", "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n", "    return ' '.join(doc)"]}, {"cell_type": "code", "execution_count": 195, "metadata": {}, "outputs": [], "source": ["token_docs = [doc_preparer(doc, sw) for doc in X_train]"]}, {"cell_type": "code", "execution_count": 196, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For demonstration purposes, we will limit our count vectorizer to 5 words (the top 5 words by frequency)."]}, {"cell_type": "code", "execution_count": 197, "metadata": {}, "outputs": [], "source": ["# Secondary train-test split to build our best model\n", "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train, test_size=.25, random_state=42)"]}, {"cell_type": "code", "execution_count": 198, "metadata": {}, "outputs": [], "source": ["cv = CountVectorizer(max_features=5)\n", "X_t_vec = cv.fit_transform(X_t)\n", "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n", "X_t_vec.columns = sorted(cv.vocabulary_)\n", "X_t_vec.set_index(y_t.index, inplace=True)"]}, {"cell_type": "code", "execution_count": 199, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>people</th>\n", "      <th>say</th>\n", "      <th>state</th>\n", "      <th>trump</th>\n", "      <th>year</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>159</th>\n", "      <td>3</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>246</th>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>7</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>640</th>\n", "      <td>0</td>\n", "      <td>4</td>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>4</td>\n", "    </tr>\n", "    <tr>\n", "      <th>809</th>\n", "      <td>2</td>\n", "      <td>10</td>\n", "      <td>2</td>\n", "      <td>0</td>\n", "      <td>7</td>\n", "    </tr>\n", "    <tr>\n", "      <th>130</th>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>148</th>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>300</th>\n", "      <td>0</td>\n", "      <td>1</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>356</th>\n", "      <td>1</td>\n", "      <td>3</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>36</th>\n", "      <td>1</td>\n", "      <td>4</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>3</td>\n", "    </tr>\n", "    <tr>\n", "      <th>895</th>\n", "      <td>1</td>\n", "      <td>7</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>6</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>562 rows \u00d7 5 columns</p>\n", "</div>"], "text/plain": ["     people  say  state  trump  year\n", "159       3    0      0      0     0\n", "246       1    0      0      7     1\n", "640       0    4      1      0     4\n", "809       2   10      2      0     7\n", "130       0    0      0      0     0\n", "..      ...  ...    ...    ...   ...\n", "148       1    0      0      0     1\n", "300       0    1      0      0     0\n", "356       1    3      0      0     0\n", "36        1    4      0      0     3\n", "895       1    7      0      0     6\n", "\n", "[562 rows x 5 columns]"]}, "execution_count": 199, "metadata": {}, "output_type": "execute_result"}], "source": ["X_t_vec"]}, {"cell_type": "code", "execution_count": 200, "metadata": {}, "outputs": [], "source": ["X_val_vec = cv.transform(X_val)\n", "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n", "X_val_vec.columns = sorted(cv.vocabulary_)\n", "X_val_vec.set_index(y_val.index, inplace=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Knowledge Check\n", "\n", "The word say shows up in our count vectorizer, but it is excluded in the stopwords.  What is going on?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Multinomial Naive Bayes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's break down MNB with our X_t_vec, and y_t arrays in mind."]}, {"cell_type": "markdown", "metadata": {}, "source": ["What are the priors for each class as calculated from these arrays?"]}, {"cell_type": "code", "execution_count": 202, "metadata": {}, "outputs": [{"data": {"text/plain": ["-0.665075161781259"]}, "execution_count": 202, "metadata": {}, "output_type": "execute_result"}], "source": ["np.log(prior_0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's train our model."]}, {"cell_type": "code", "execution_count": 203, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'alpha': 1.0,\n", " 'fit_prior': True,\n", " 'class_prior': None,\n", " 'n_features_': 5,\n", " 'classes_': array([0, 1]),\n", " 'class_count_': array([273., 289.]),\n", " 'feature_count_': array([[ 211., 1419.,  371.,  283.,  348.],\n", "        [ 385.,  241.,  111.,  152.,  264.]]),\n", " 'feature_log_prob_': array([[-2.52081091, -0.61898504, -1.95850333, -2.22842295, -2.02232526],\n", "        [-1.09861229, -1.56551193, -2.33595079, -2.02401174, -1.47471983]]),\n", " 'class_log_prior_': array([-0.72203005, -0.66507516])}"]}, "execution_count": 203, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.naive_bayes import MultinomialNB\n", "\n", "mnb = MultinomialNB()\n", "\n", "mnb.fit(X_t_vec, y_t)\n", "mnb.__dict__"]}, {"cell_type": "code", "execution_count": 204, "metadata": {}, "outputs": [], "source": ["# https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html"]}, {"cell_type": "code", "execution_count": 205, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>people</th>\n", "      <th>say</th>\n", "      <th>state</th>\n", "      <th>trump</th>\n", "      <th>year</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>520</th>\n", "      <td>1</td>\n", "      <td>7</td>\n", "      <td>6</td>\n", "      <td>0</td>\n", "      <td>1</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["     people  say  state  trump  year\n", "520       1    7      6      0     1"]}, "execution_count": 205, "metadata": {}, "output_type": "execute_result"}], "source": ["random_sample = X_val_vec.sample(1, random_state=40)\n", "random_sample.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our Likelihoods would look like so:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$ \\Large P(satire|count\\_people, count\\_say...count\\_year)$$\n", "\n", "$$ \\Large P(not\\_satire|count\\_people, count\\_go...count\\_year)$$"]}, {"cell_type": "code", "execution_count": 206, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["520   -20.627051\n", "dtype: float64 520   -27.54762\n", "dtype: float64\n"]}], "source": ["likelihood_nosat = mnb.feature_log_prob_[0]*random_sample\n", "likelihood_sat =  mnb.feature_log_prob_[1]*random_sample\n", "likelihood_nosat = likelihood_nosat.agg(sum, axis=1)\n", "likelihood_sat = likelihood_sat.agg(sum, axis=1)\n", "\n", "print(likelihood_nosat, likelihood_sat)"]}, {"cell_type": "code", "execution_count": 207, "metadata": {}, "outputs": [{"data": {"text/plain": ["520   -21.349081\n", "dtype: float64"]}, "execution_count": 207, "metadata": {}, "output_type": "execute_result"}], "source": ["likelihood_nosat + mnb.class_log_prior_[0]"]}, {"cell_type": "code", "execution_count": 208, "metadata": {}, "outputs": [{"data": {"text/plain": ["520   -28.212696\n", "dtype: float64"]}, "execution_count": 208, "metadata": {}, "output_type": "execute_result"}], "source": ["likelihood_sat + mnb.class_log_prior_[1]"]}, {"cell_type": "code", "execution_count": 209, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0])"]}, "execution_count": 209, "metadata": {}, "output_type": "execute_result"}], "source": ["mnb.predict(random_sample)"]}, {"cell_type": "code", "execution_count": 210, "metadata": {}, "outputs": [{"data": {"text/plain": ["520    0\n", "Name: target, dtype: int64"]}, "execution_count": 210, "metadata": {}, "output_type": "execute_result"}], "source": ["y_val.loc[random_sample.index]"]}, {"cell_type": "code", "execution_count": 211, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n", "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n", "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n", "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n", "       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n", "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n", "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n", "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n", "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])"]}, "execution_count": 211, "metadata": {}, "output_type": "execute_result"}], "source": ["\n", "y_hat = mnb.predict(X_val_vec)\n", "y_hat"]}, {"cell_type": "code", "execution_count": 212, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.8297872340425532"]}, "execution_count": 212, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n", "\n", "accuracy_score(y_val, y_hat)"]}, {"cell_type": "code", "execution_count": 213, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.8202247191011236"]}, "execution_count": 213, "metadata": {}, "output_type": "execute_result"}], "source": ["f1_score(y_val, y_hat)"]}, {"cell_type": "code", "execution_count": 214, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[83, 16],\n", "       [16, 73]])"]}, "execution_count": 214, "metadata": {}, "output_type": "execute_result"}], "source": ["confusion_matrix(y_val, y_hat)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That performs very well for only having 5 features."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's see what happens when we increase our feature set"]}, {"cell_type": "code", "execution_count": 215, "metadata": {}, "outputs": [{"data": {"text/plain": ["(562, 14819)"]}, "execution_count": 215, "metadata": {}, "output_type": "execute_result"}], "source": ["cv = CountVectorizer()\n", "X_t_vec = cv.fit_transform(X_t)\n", "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n", "X_t_vec.columns = sorted(cv.vocabulary_)\n", "X_t_vec.set_index(y_t.index, inplace=True)\n", "X_t_vec.shape"]}, {"cell_type": "code", "execution_count": 216, "metadata": {}, "outputs": [], "source": ["X_val_vec = cv.transform(X_val)\n", "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n", "X_val_vec.columns = sorted(cv.vocabulary_)\n", "X_val_vec.set_index(y_val.index, inplace=True)"]}, {"cell_type": "code", "execution_count": 217, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n", "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n", "       1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n", "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n", "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n", "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n", "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n", "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n", "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0])"]}, "execution_count": 217, "metadata": {}, "output_type": "execute_result"}], "source": ["mnb = MultinomialNB()\n", "\n", "mnb.fit(X_t_vec, y_t)\n", "y_hat = mnb.predict(X_val_vec)\n", "y_hat"]}, {"cell_type": "code", "execution_count": 218, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.9627659574468085"]}, "execution_count": 218, "metadata": {}, "output_type": "execute_result"}], "source": ["accuracy_score(y_val, y_hat)"]}, {"cell_type": "code", "execution_count": 219, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.96045197740113"]}, "execution_count": 219, "metadata": {}, "output_type": "execute_result"}], "source": ["f1_score(y_val, y_hat)"]}, {"cell_type": "code", "execution_count": 220, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[96,  3],\n", "       [ 4, 85]])"]}, "execution_count": 220, "metadata": {}, "output_type": "execute_result"}], "source": ["confusion_matrix(y_val, y_hat)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That performs very well. \n", "\n", "Let's see whether or not we can maintain that level of accuracy with less words."]}, {"cell_type": "code", "execution_count": 221, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.9378531073446328"]}, "execution_count": 221, "metadata": {}, "output_type": "execute_result"}], "source": ["cv = CountVectorizer(min_df=.05, max_df=.95)\n", "X_t_vec = cv.fit_transform(X_t)\n", "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n", "X_t_vec.columns = sorted(cv.vocabulary_)\n", "X_t_vec.set_index(y_t.index, inplace=True)\n", "\n", "X_val_vec = cv.transform(X_val)\n", "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n", "X_val_vec.columns = sorted(cv.vocabulary_)\n", "X_val_vec.set_index(y_val.index, inplace=True)\n", "\n", "mnb = MultinomialNB()\n", "\n", "mnb.fit(X_t_vec, y_t)\n", "y_hat = mnb.predict(X_val_vec)\n", "\n", "f1_score(y_val, y_hat)"]}, {"cell_type": "code", "execution_count": 222, "metadata": {}, "outputs": [{"data": {"text/plain": ["(562, 650)"]}, "execution_count": 222, "metadata": {}, "output_type": "execute_result"}], "source": ["X_t_vec.shape"]}, {"cell_type": "code", "execution_count": 223, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.9680851063829787"]}, "execution_count": 223, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "\n", "rf = RandomForestClassifier()\n", "rf.fit(X_t_vec, y_t)\n", "rf.score(X_val_vec, y_val)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bonus NLP EDA"]}, {"cell_type": "code", "execution_count": 122, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>Unnamed: 0</th>\n", "      <th>name</th>\n", "      <th>policy</th>\n", "      <th>candidate</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>0</td>\n", "      <td>100% Clean Energy for America</td>\n", "      <td>As published on Medium on September 3rd, 2019:...</td>\n", "      <td>warren</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1</td>\n", "      <td>A Comprehensive Agenda to Boost America\u2019s Smal...</td>\n", "      <td>Small businesses are the heart of our economy....</td>\n", "      <td>warren</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2</td>\n", "      <td>A Fair and Welcoming Immigration System</td>\n", "      <td>As published on Medium on July 11th, 2019:\\nIm...</td>\n", "      <td>warren</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>3</td>\n", "      <td>A Fair Workweek for America\u2019s Part-Time Workers</td>\n", "      <td>Working families all across the country are ge...</td>\n", "      <td>warren</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>4</td>\n", "      <td>A Great Public School Education for Every Student</td>\n", "      <td>I attended public school growing up in Oklahom...</td>\n", "      <td>warren</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   Unnamed: 0                                               name  \\\n", "0           0                      100% Clean Energy for America   \n", "1           1  A Comprehensive Agenda to Boost America\u2019s Smal...   \n", "2           2            A Fair and Welcoming Immigration System   \n", "3           3    A Fair Workweek for America\u2019s Part-Time Workers   \n", "4           4  A Great Public School Education for Every Student   \n", "\n", "                                              policy candidate  \n", "0  As published on Medium on September 3rd, 2019:...    warren  \n", "1  Small businesses are the heart of our economy....    warren  \n", "2  As published on Medium on July 11th, 2019:\\nIm...    warren  \n", "3  Working families all across the country are ge...    warren  \n", "4  I attended public school growing up in Oklahom...    warren  "]}, "execution_count": 122, "metadata": {}, "output_type": "execute_result"}], "source": ["policies = pd.read_csv('data/2020_policies_feb_24.csv')\n", "policies.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Question set 1:\n", "After remove punctuation and ridding the text of numbers and other low semantic value text, answer the following questions.\n", "\n", "1. Which document has the greatest average word length?\n", "2. What is the average word length of the entire corpus?\n", "3. Which is greater, the average word length for the documents in the Warren or Sanders campaigns? \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Proceed through the remaining standard preprocessing steps in whatever manner you see fit. Make sure to:\n", "- Make text lowercase\n", "- Remove stopwords\n", "- Stem or lemmatize"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Question set 2:\n", "1. What are the most common words across the corpus?\n", "2. What are the most common words across each campaign?\n", "\n", "> in order to answer these questions, you may find the nltk FreqDist function helpful.\n", "\n", "3. Use the FreqDist plot method to make a frequency plot for the corpus as a whole.  \n", "4. Based on that plot, should any more words be added to our stopword library?\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Question set 3:\n", "\n", "1. What are the most common bigrams in the corpus?\n", "2. What are the most common bigrams in the Warren campain and the Sanders campaign, respectively?\n", "3. Answer questions 1 and 2 for trigrams.\n", "\n", "> Hint: You may find it useful to leverage the nltk.collocations functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["After answering the questions, transform the data into a document term matrix using either CountVectorizor or TFIDF.  \n", "\n", "Run a Multinomial Naive Bayes classifier and judge how accurately our models can separate documents from the two campaigns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}